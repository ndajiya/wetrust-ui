{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b517a67b3a8f47f8af0464d498dab31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c99b471edb644ca090a87621a1a9da5d",
              "IPY_MODEL_63bc70d497ef45e496640ea8fd3fd170",
              "IPY_MODEL_4587101068984ee0a8e3531dcf4826c1"
            ],
            "layout": "IPY_MODEL_d0125fbbe1af483e830c2b7bc45e7012"
          }
        },
        "c99b471edb644ca090a87621a1a9da5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c06c8f525b424a8774b8a4b9f361d7",
            "placeholder": "​",
            "style": "IPY_MODEL_2fb5cb8dd9e74c1cad6874aa6825da53",
            "value": "model.safetensors: 100%"
          }
        },
        "63bc70d497ef45e496640ea8fd3fd170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2f0cb65ffa7414f856684aafc19c103",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69233e9022f54ce6ad159337e1e4dcb1",
            "value": 548105171
          }
        },
        "4587101068984ee0a8e3531dcf4826c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce454bf2c35a4de394dfd9485dc49e34",
            "placeholder": "​",
            "style": "IPY_MODEL_83c2aecced3d4fd4980eae17cc46bfe1",
            "value": " 548M/548M [00:11&lt;00:00, 55.8MB/s]"
          }
        },
        "d0125fbbe1af483e830c2b7bc45e7012": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70c06c8f525b424a8774b8a4b9f361d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fb5cb8dd9e74c1cad6874aa6825da53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2f0cb65ffa7414f856684aafc19c103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69233e9022f54ce6ad159337e1e4dcb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce454bf2c35a4de394dfd9485dc49e34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83c2aecced3d4fd4980eae17cc46bfe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "982ecb68b727414790f676c90be81301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f20702908e044f1c9f72e4bba77e278d",
              "IPY_MODEL_dcc92e1e9232463081ffa3240958568f",
              "IPY_MODEL_63cdc1792aea496eac76b0d1fbef1229"
            ],
            "layout": "IPY_MODEL_16a7ff3622c2404da04d3abcb05812ae"
          }
        },
        "f20702908e044f1c9f72e4bba77e278d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82dc7cba0bee46f4bdbd7b7edd550f78",
            "placeholder": "​",
            "style": "IPY_MODEL_d61ac84d6b2d4cc5b0f4d578bcc5c193",
            "value": "generation_config.json: 100%"
          }
        },
        "dcc92e1e9232463081ffa3240958568f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3b3818c4f6d48179526f9ba1bf79f18",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dead2a001754ccab57cca894d2e4ac2",
            "value": 124
          }
        },
        "63cdc1792aea496eac76b0d1fbef1229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74dc6434f56340c587303fc368bb5b6d",
            "placeholder": "​",
            "style": "IPY_MODEL_c6ec623c36e94a3183be8f658a2aa3ad",
            "value": " 124/124 [00:00&lt;00:00, 3.12kB/s]"
          }
        },
        "16a7ff3622c2404da04d3abcb05812ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82dc7cba0bee46f4bdbd7b7edd550f78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d61ac84d6b2d4cc5b0f4d578bcc5c193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3b3818c4f6d48179526f9ba1bf79f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dead2a001754ccab57cca894d2e4ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74dc6434f56340c587303fc368bb5b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6ec623c36e94a3183be8f658a2aa3ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9d3992ded4940dfa5ea16215e9614fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69fd841989f243908da85fab9cc83933",
              "IPY_MODEL_0038a7b7405f455194b072f118904af5",
              "IPY_MODEL_6170768e883544d193dedc3a8ae7cc49"
            ],
            "layout": "IPY_MODEL_01cb90d9d1994b73a8b538c074c0dc5e"
          }
        },
        "69fd841989f243908da85fab9cc83933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a3a22c376544028b5c1fd152321d428",
            "placeholder": "​",
            "style": "IPY_MODEL_29cf60e7124141ceb6b9d3ffa931cee0",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "0038a7b7405f455194b072f118904af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_894728a51d704202b7cd7e78710b8913",
            "max": 4813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94041725847847b4810448a6aac2dc75",
            "value": 4813
          }
        },
        "6170768e883544d193dedc3a8ae7cc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f08c0db6ed3a45e1a5c1c87d82aceee2",
            "placeholder": "​",
            "style": "IPY_MODEL_8aee0305df6c49fb854b88b4778b7b4c",
            "value": " 4813/4813 [00:02&lt;00:00, 2184.90 examples/s]"
          }
        },
        "01cb90d9d1994b73a8b538c074c0dc5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a3a22c376544028b5c1fd152321d428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29cf60e7124141ceb6b9d3ffa931cee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "894728a51d704202b7cd7e78710b8913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94041725847847b4810448a6aac2dc75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f08c0db6ed3a45e1a5c1c87d82aceee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aee0305df6c49fb854b88b4778b7b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndajiya/wetrust-ui/blob/master/shrek_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQx-7PCyeMBv",
        "outputId": "37ca9cad-8106-44ab-cbcb-6dd93f4332c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, bitsandbytes\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.4\n",
            "    Uninstalling tokenizers-0.21.4:\n",
            "      Successfully uninstalled tokenizers-0.21.4\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.55.4\n",
            "    Uninstalling transformers-4.55.4:\n",
            "      Successfully uninstalled transformers-4.55.4\n",
            "Successfully installed bitsandbytes-0.47.0 tokenizers-0.22.0 transformers-4.56.0\n",
            "Collecting torch-pruning\n",
            "  Downloading torch_pruning-1.6.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torch-pruning) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-pruning) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torch-pruning) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torch-pruning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torch-pruning) (3.0.2)\n",
            "Downloading torch_pruning-1.6.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-pruning\n",
            "Successfully installed torch-pruning-1.6.0\n",
            "Collecting git+https://github.com/huggingface/peft\n",
            "  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-f6fh1hdm\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-f6fh1hdm\n",
            "  Resolved https://github.com/huggingface/peft to commit 2ea5377ee3069fa5aa49ed3930ac52bc706aed85\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (4.56.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (1.10.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (0.6.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (1.1.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.17.2.dev0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.17.2.dev0) (0.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.17.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.17.2.dev0) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2025.8.3)\n",
            "Building wheels for collected packages: peft\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.17.2.dev0-py3-none-any.whl size=527383 sha256=66fd2e9c8edfe61a03b43e224f7932ba2a43787db53489ed419b39c433cbde23\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m2t1kuhv/wheels/92/c9/f9/6d1893af725a9b89369aa9416c272f2d512491873dd9db54cd\n",
            "Successfully built peft\n",
            "Installing collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.17.1\n",
            "    Uninstalling peft-0.17.1:\n",
            "      Successfully uninstalled peft-0.17.1\n",
            "Successfully installed peft-0.17.2.dev0\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,961 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,790 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,232 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,374 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,579 kB]\n",
            "Fetched 28.3 MB in 5s (6,215 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers datasets torch accelerate peft bitsandbytes sentencepiece\n",
        "!pip install torch-pruning\n",
        "!pip install git+https://github.com/huggingface/peft\n",
        "!pip install einops\n",
        "\n",
        "!apt-get update && apt-get install build-essential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = ['shrek.txt' , 'shitposts.txt']\n",
        "\n",
        "with open('merged.txt' , 'w') as outfile:\n",
        "  for fname in files:\n",
        "    try:\n",
        "      with open(fname , 'r' , encoding = 'utf-8') as infile:\n",
        "        outfile.write(infile.read() + \"\\n\\n\")\n",
        "    except FileNotFoundError :\n",
        "      print(f\"[!] {fname} not found\")\n",
        "  print(\"text merged\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRFNHBCS3zDt",
        "outputId": "19d2ed1d-3a7f-4cfc-d7ff-5e85627eb8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text merged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer , GPT2LMHeadModel , Trainer , TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token =tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "dataset = load_dataset('text' , data_files = 'merged.txt')\n",
        "def tokenize_function(examples) :\n",
        "  return tokenizer(examples[\"text\"] , truncation = True , padding = True , max_length = 128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "b517a67b3a8f47f8af0464d498dab31b",
            "c99b471edb644ca090a87621a1a9da5d",
            "63bc70d497ef45e496640ea8fd3fd170",
            "4587101068984ee0a8e3531dcf4826c1",
            "d0125fbbe1af483e830c2b7bc45e7012",
            "70c06c8f525b424a8774b8a4b9f361d7",
            "2fb5cb8dd9e74c1cad6874aa6825da53",
            "c2f0cb65ffa7414f856684aafc19c103",
            "69233e9022f54ce6ad159337e1e4dcb1",
            "ce454bf2c35a4de394dfd9485dc49e34",
            "83c2aecced3d4fd4980eae17cc46bfe1",
            "982ecb68b727414790f676c90be81301",
            "f20702908e044f1c9f72e4bba77e278d",
            "dcc92e1e9232463081ffa3240958568f",
            "63cdc1792aea496eac76b0d1fbef1229",
            "16a7ff3622c2404da04d3abcb05812ae",
            "82dc7cba0bee46f4bdbd7b7edd550f78",
            "d61ac84d6b2d4cc5b0f4d578bcc5c193",
            "b3b3818c4f6d48179526f9ba1bf79f18",
            "8dead2a001754ccab57cca894d2e4ac2",
            "74dc6434f56340c587303fc368bb5b6d",
            "c6ec623c36e94a3183be8f658a2aa3ad",
            "b9d3992ded4940dfa5ea16215e9614fb",
            "69fd841989f243908da85fab9cc83933",
            "0038a7b7405f455194b072f118904af5",
            "6170768e883544d193dedc3a8ae7cc49",
            "01cb90d9d1994b73a8b538c074c0dc5e",
            "0a3a22c376544028b5c1fd152321d428",
            "29cf60e7124141ceb6b9d3ffa931cee0",
            "894728a51d704202b7cd7e78710b8913",
            "94041725847847b4810448a6aac2dc75",
            "f08c0db6ed3a45e1a5c1c87d82aceee2",
            "8aee0305df6c49fb854b88b4778b7b4c"
          ]
        },
        "id": "rXasHFsE4dX-",
        "outputId": "1b9f355a-363e-46c8-f667-e4989cd503a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b517a67b3a8f47f8af0464d498dab31b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "982ecb68b727414790f676c90be81301"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/4813 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9d3992ded4940dfa5ea16215e9614fb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ppG6_Gi-GqG",
        "outputId": "4283d176-5d39-4a15-ea3c-faac29f828f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-shrek-chaos\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    save_steps=100,\n",
        "    logging_steps=50,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_dir=\"logs\",\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=1,\n",
        "\n",
        "    disable_tqdm=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"training..\")\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"gpt2-shrek-chaos\")\n",
        "tokenizer.save_pretrained(\"gpt2-shrek-chaos\")\n",
        "\n",
        "print(\"saved to 'gpt2-shrek-chaos'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "collapsed": true,
        "id": "teYq39CY5i5N",
        "outputId": "610f5c14-d63f-4b9c-fbec-a97dc93d96ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='602' max='602' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [602/602 05:26, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.205100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.826600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.949900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.776800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.374700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.577800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.907300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.935700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.968000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.010100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.771900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.777400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved to 'gpt2-shrek-chaos'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\n",
        "    \"gpt2-shrek-chaos\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    dtype=torch.float16,\n",
        ")\n"
      ],
      "metadata": {
        "id": "KHxhHkNg5kEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-shrek-chaos\")\n",
        "model.eval()\n",
        "weighsToSparce = 0.5\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "        with torch.no_grad():\n",
        "            w = module.weight.data\n",
        "            mask = torch.rand(w.shape) > weighsToSparce\n",
        "            w *= mask.to(w.device)\n",
        "\n",
        "print(\"Model weights sparsified (\" , weighsToSparce , \") zeroed out)\")\n",
        "\n",
        "model.save_pretrained(\"gpt2-shrek-chaos-sparse\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_9hsfxjCLJO",
        "outputId": "f32abe84-32fb-4429-a39f-2950f56cd19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights sparsified (50% zeroed out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral-common\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "259lq8S1WN0m",
        "outputId": "692e4836-7f88-4d71-b845-0b04e08527e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistral-common\n",
            "  Downloading mistral_common-1.8.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.11.7)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.25.1)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.15.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (0.11.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.0.2)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common)\n",
            "  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.27.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.4.1)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2025.8.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->mistral-common) (2024.11.6)\n",
            "Downloading mistral_common-1.8.4-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycountry, pydantic-extra-types, mistral-common\n",
            "Successfully installed mistral-common-1.8.4 pycountry-24.6.1 pydantic-extra-types-2.10.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-shrek-chaos\")\n",
        "tokenizer.save_pretrained(\"gpt2-shrek-chaos\")\n",
        "\n",
        "!rm -rf llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!pip install sentencepiece tokenizers gguf mistral-common\n",
        "\n",
        "!cd llama.cpp && mkdir -p build && cd build && \\\n",
        "cmake .. \\\n",
        "-DCMAKE_BUILD_TYPE=Release \\\n",
        "&& make -j\n",
        "\n",
        "!cp -r gpt2-shrek-chaos llama.cpp/models/gpt2-shrek-chaos\n",
        "\n",
        "!cd llama.cpp && python3 convert_hf_to_gguf.py models/gpt2-shrek-chaos --outfile models/gpt2-shrek-chaos.gguf\n",
        "\n",
        "!cd llama.cpp && ./build/bin/llama-quantize models/gpt2-shrek-chaos.gguf models/gpt2-shrek-chaos-Q2_K.gguf Q2_K\n",
        "\n",
        "!ls -lh llama.cpp/models/*.gguf\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT0GhxbIG2hk",
        "outputId": "d0c819f5-ab19-4ae1-e65c-d38205a37700",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 60791, done.\u001b[K\n",
            "remote: Counting objects: 100% (122/122), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 60791 (delta 86), reused 32 (delta 32), pack-reused 60669 (from 4)\u001b[K\n",
            "Receiving objects: 100% (60791/60791), 151.29 MiB | 17.78 MiB/s, done.\n",
            "Resolving deltas: 100% (44115/44115), done.\n",
            "Updating files: 100% (1451/1451), done.\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.0)\n",
            "Requirement already satisfied: gguf in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: mistral-common in /usr/local/lib/python3.12/dist-packages (1.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from gguf) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from gguf) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from gguf) (4.67.1)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.11.7)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.25.1)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.15.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (0.11.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.32.4)\n",
            "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common) (2.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.8)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.27.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.4.1)\n",
            "Requirement already satisfied: pycountry>=23 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common) (24.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2025.8.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->mistral-common) (2024.11.6)\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.0.6373\n",
            "-- ggml commit:  0fce7a12\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (1.9s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  7%] Built target build_info\n",
            "[  7%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  7%] Built target sha1\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[  9%] Built target llama-gemma3-cli\n",
            "[  9%] Built target llama-minicpmv-cli\n",
            "[  9%] Built target sha256\n",
            "[  9%] Built target llama-qwen2vl-cli\n",
            "[  9%] Built target llama-llava-cli\n",
            "[  9%] Built target xxhash\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  9%] Built target ggml-base\n",
            "[  9%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 15%] Built target ggml-cpu\n",
            "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 15%] Built target ggml\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 27%] Built target llama-gguf\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 27%] Built target llama-gguf-hash\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 27%] Built target llama\n",
            "[ 27%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 35%] Built target test-c\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 35%] Built target llama-simple\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 36%] Built target llama-simple-chat\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 37%] Built target mtmd\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 38%] Built target common\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 66%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 71%] Built target test-model-load-cancel\n",
            "[ 71%] Built target test-mtmd-c-api\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 71%] Built target test-log\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 71%] Built target test-autorelease\n",
            "[ 71%] Built target test-rope\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 72%] Built target test-barrier\n",
            "[ 72%] Built target test-quantize-fns\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 74%] Built target llama-q8dot\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 74%] Built target llama-logits\n",
            "[ 74%] Built target llama-vdot\n",
            "[ 74%] Built target llama-lookup-merge\n",
            "[ 74%] Built target test-gbnf-validator\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 74%] Built target test-tokenizer-1-spm\n",
            "[ 74%] Built target test-tokenizer-1-bpe\n",
            "[ 74%] Built target llama-tokenize\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 75%] Built target llama-gguf-split\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 76%] Built target test-grammar-parser\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 77%] Built target test-sampling\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 79%] Built target test-tokenizer-0\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 82%] Built target llama-finetune\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 83%] Built target llama-lookup-create\n",
            "[ 83%] Built target test-llama-grammar\n",
            "[ 83%] Built target test-quantize-perf\n",
            "[ 83%] Built target llama-gen-docs\n",
            "[ 83%] Built target llama-eval-callback\n",
            "[ 83%] Built target test-regex-partial\n",
            "[ 83%] Built target test-thread-safety\n",
            "[ 83%] Built target llama-batched\n",
            "[ 83%] Built target llama-save-load-state\n",
            "[ 83%] Built target llama-gritlm\n",
            "[ 83%] Built target llama-speculative-simple\n",
            "[ 83%] Built target llama-lookup-stats\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 85%] Built target llama-passkey\n",
            "[ 85%] Built target llama-batched-bench\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 85%] Built target llama-lookup\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 87%] Built target test-opt\n",
            "[ 87%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 88%] Built target test-arg-parser\n",
            "[ 88%] Built target llama-parallel\n",
            "[ 88%] Built target llama-embedding\n",
            "[ 88%] Built target llama-lookahead\n",
            "[ 88%] Built target test-gguf\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 89%] Built target llama-quantize\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 91%] Built target llama-retrieval\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 92%] Built target llama-export-lora\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 92%] Built target llama-mtmd-cli\n",
            "[ 92%] Built target test-chat-template\n",
            "[ 92%] Built target llama-diffusion-cli\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 93%] Built target llama-cvector-generator\n",
            "[ 93%] Built target llama-speculative\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 93%] Built target test-json-partial\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 94%] Built target llama-cli\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 94%] Built target llama-perplexity\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 95%] Built target test-quantize-stats\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 95%] Built target test-grammar-integration\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 96%] Built target llama-run\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 97%] Built target test-chat-parser\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 99%] Built target test-json-schema-to-grammar\n",
            "[ 99%] Built target llama-imatrix\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[100%] Built target llama-bench\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[100%] Built target llama-tts\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[100%] Built target test-chat\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[100%] Built target test-backend-ops\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n",
            "INFO:hf-to-gguf:Loading model: gpt2-shrek-chaos\n",
            "INFO:hf-to-gguf:Model architecture: GPT2LMHeadModel\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
            "INFO:hf-to-gguf:blk.0.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.1.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.10.attn_qkv.bias,      torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.bias,   torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.bias,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.bias,        torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.11.attn_qkv.bias,      torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.bias,   torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.bias,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.bias,        torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.2.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.3.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.4.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.5.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.6.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.7.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.8.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.9.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:output_norm.bias,          torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:position_embd.weight,      torch.float32 --> F32, shape = {768, 1024}\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.float32 --> F16, shape = {768, 50257}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:gguf.vocab:Adding 50000 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 50256\n",
            "INFO:gguf.vocab:Setting special token type eos to 50256\n",
            "INFO:gguf.vocab:Setting special token type unk to 50256\n",
            "INFO:gguf.vocab:Setting special token type pad to 50256\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:models/gpt2-shrek-chaos.gguf: n_tensors = 148, total_size = 250.7M\n",
            "Writing: 100% 251M/251M [00:03<00:00, 66.8Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to models/gpt2-shrek-chaos.gguf\n",
            "main: build = 6373 (0fce7a12)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'models/gpt2-shrek-chaos.gguf' to 'models/gpt2-shrek-chaos-Q2_K.gguf' as Q2_K\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 148 tensors from models/gpt2-shrek-chaos.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gpt2 Shrek Chaos\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 124M\n",
            "llama_model_loader: - kv   4:                           gpt2.block_count u32              = 12\n",
            "llama_model_loader: - kv   5:                        gpt2.context_length u32              = 1024\n",
            "llama_model_loader: - kv   6:                      gpt2.embedding_length u32              = 768\n",
            "llama_model_loader: - kv   7:                   gpt2.feed_forward_length u32              = 3072\n",
            "llama_model_loader: - kv   8:                  gpt2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv   9:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = gpt-2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,50257]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,50257]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 50256\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - type  f32:   99 tensors\n",
            "llama_model_loader: - type  f16:   49 tensors\n",
            "[   1/ 148]                     output_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[   2/ 148]                   output_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[   3/ 148]                 position_embd.weight - [  768,  1024,     1,     1], type =    f32, size =    3.000 MB\n",
            "[   4/ 148]                    token_embd.weight - [  768, 50257,     1,     1], type =    f16, converting to q6_K .. size =    73.62 MiB ->    30.20 MiB\n",
            "[   5/ 148]                 blk.0.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[   6/ 148]               blk.0.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[   7/ 148]               blk.0.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[   8/ 148]             blk.0.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[   9/ 148]                  blk.0.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  10/ 148]                blk.0.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  11/ 148]                  blk.0.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  12/ 148]                blk.0.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  13/ 148]                  blk.0.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  14/ 148]                blk.0.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  15/ 148]                    blk.0.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  16/ 148]                  blk.0.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  17/ 148]                 blk.1.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  18/ 148]               blk.1.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  19/ 148]               blk.1.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  20/ 148]             blk.1.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  21/ 148]                  blk.1.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  22/ 148]                blk.1.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  23/ 148]                  blk.1.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  24/ 148]                blk.1.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  25/ 148]                  blk.1.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  26/ 148]                blk.1.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  27/ 148]                    blk.1.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  28/ 148]                  blk.1.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  29/ 148]                 blk.2.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  30/ 148]               blk.2.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  31/ 148]               blk.2.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  32/ 148]             blk.2.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  33/ 148]                  blk.2.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  34/ 148]                blk.2.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  35/ 148]                  blk.2.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  36/ 148]                blk.2.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  37/ 148]                  blk.2.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  38/ 148]                blk.2.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  39/ 148]                    blk.2.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  40/ 148]                  blk.2.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  41/ 148]                 blk.3.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  42/ 148]               blk.3.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  43/ 148]               blk.3.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  44/ 148]             blk.3.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  45/ 148]                  blk.3.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  46/ 148]                blk.3.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  47/ 148]                  blk.3.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  48/ 148]                blk.3.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  49/ 148]                  blk.3.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  50/ 148]                blk.3.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  51/ 148]                    blk.3.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  52/ 148]                  blk.3.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  53/ 148]                 blk.4.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  54/ 148]               blk.4.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  55/ 148]               blk.4.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  56/ 148]             blk.4.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  57/ 148]                  blk.4.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  58/ 148]                blk.4.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  59/ 148]                  blk.4.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  60/ 148]                blk.4.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  61/ 148]                  blk.4.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  62/ 148]                blk.4.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  63/ 148]                    blk.4.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  64/ 148]                  blk.4.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  65/ 148]                 blk.5.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  66/ 148]               blk.5.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  67/ 148]               blk.5.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  68/ 148]             blk.5.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  69/ 148]                  blk.5.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  70/ 148]                blk.5.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  71/ 148]                  blk.5.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  72/ 148]                blk.5.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  73/ 148]                  blk.5.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  74/ 148]                blk.5.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  75/ 148]                    blk.5.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  76/ 148]                  blk.5.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  77/ 148]                 blk.6.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  78/ 148]               blk.6.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  79/ 148]               blk.6.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  80/ 148]             blk.6.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  81/ 148]                  blk.6.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  82/ 148]                blk.6.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  83/ 148]                  blk.6.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  84/ 148]                blk.6.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  85/ 148]                  blk.6.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  86/ 148]                blk.6.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  87/ 148]                    blk.6.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  88/ 148]                  blk.6.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  89/ 148]                 blk.7.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  90/ 148]               blk.7.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  91/ 148]               blk.7.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  92/ 148]             blk.7.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  93/ 148]                  blk.7.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[  94/ 148]                blk.7.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  95/ 148]                  blk.7.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  96/ 148]                blk.7.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  97/ 148]                  blk.7.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  98/ 148]                blk.7.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  99/ 148]                    blk.7.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 100/ 148]                  blk.7.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[ 101/ 148]                 blk.8.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 102/ 148]               blk.8.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 103/ 148]               blk.8.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 104/ 148]             blk.8.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[ 105/ 148]                  blk.8.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 106/ 148]                blk.8.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[ 107/ 148]                  blk.8.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 108/ 148]                blk.8.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[ 109/ 148]                  blk.8.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 110/ 148]                blk.8.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 111/ 148]                    blk.8.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 112/ 148]                  blk.8.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[ 113/ 148]                 blk.9.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 114/ 148]               blk.9.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 115/ 148]               blk.9.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 116/ 148]             blk.9.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[ 117/ 148]                  blk.9.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 118/ 148]                blk.9.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[ 119/ 148]                  blk.9.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 120/ 148]                blk.9.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[ 121/ 148]                  blk.9.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 122/ 148]                blk.9.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 123/ 148]                    blk.9.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 124/ 148]                  blk.9.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[ 125/ 148]                blk.10.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 126/ 148]              blk.10.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 127/ 148]              blk.10.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 128/ 148]            blk.10.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[ 129/ 148]                 blk.10.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 130/ 148]               blk.10.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[ 131/ 148]                 blk.10.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 132/ 148]               blk.10.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[ 133/ 148]                 blk.10.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 134/ 148]               blk.10.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 135/ 148]                   blk.10.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 136/ 148]                 blk.10.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[ 137/ 148]                blk.11.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 138/ 148]              blk.11.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 139/ 148]              blk.11.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 140/ 148]            blk.11.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[ 141/ 148]                 blk.11.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MB\n",
            "[ 142/ 148]               blk.11.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[ 143/ 148]                 blk.11.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 144/ 148]               blk.11.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[ 145/ 148]                 blk.11.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 146/ 148]               blk.11.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 147/ 148]                   blk.11.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 148/ 148]                 blk.11.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "llama_model_quantize_impl: model size  =   239.08 MB\n",
            "llama_model_quantize_impl: quant size  =    63.66 MB\n",
            "\n",
            "main: quantize time =  7637.06 ms\n",
            "main:    total time =  7637.06 ms\n",
            "-rw-r--r-- 1 root root 4.7M Sep  3 20:52 llama.cpp/models/ggml-vocab-aquila.gguf\n",
            "-rw-r--r-- 1 root root 1.3M Sep  3 20:52 llama.cpp/models/ggml-vocab-baichuan.gguf\n",
            "-rw-r--r-- 1 root root 613K Sep  3 20:52 llama.cpp/models/ggml-vocab-bert-bge.gguf\n",
            "-rw-r--r-- 1 root root  11M Sep  3 20:52 llama.cpp/models/ggml-vocab-command-r.gguf\n",
            "-rw-r--r-- 1 root root 1.2M Sep  3 20:52 llama.cpp/models/ggml-vocab-deepseek-coder.gguf\n",
            "-rw-r--r-- 1 root root 3.8M Sep  3 20:52 llama.cpp/models/ggml-vocab-deepseek-llm.gguf\n",
            "-rw-r--r-- 1 root root 2.2M Sep  3 20:52 llama.cpp/models/ggml-vocab-falcon.gguf\n",
            "-rw-r--r-- 1 root root 1.7M Sep  3 20:52 llama.cpp/models/ggml-vocab-gpt-2.gguf\n",
            "-rw-r--r-- 1 root root 1.7M Sep  3 20:52 llama.cpp/models/ggml-vocab-gpt-neox.gguf\n",
            "-rw-r--r-- 1 root root 7.5M Sep  3 20:52 llama.cpp/models/ggml-vocab-llama-bpe.gguf\n",
            "-rw-r--r-- 1 root root 707K Sep  3 20:52 llama.cpp/models/ggml-vocab-llama-spm.gguf\n",
            "-rw-r--r-- 1 root root 1.7M Sep  3 20:52 llama.cpp/models/ggml-vocab-mpt.gguf\n",
            "-rw-r--r-- 1 root root 6.6M Sep  3 20:52 llama.cpp/models/ggml-vocab-nomic-bert-moe.gguf\n",
            "-rw-r--r-- 1 root root 710K Sep  3 20:52 llama.cpp/models/ggml-vocab-phi-3.gguf\n",
            "-rw-r--r-- 1 root root 5.7M Sep  3 20:52 llama.cpp/models/ggml-vocab-qwen2.gguf\n",
            "-rw-r--r-- 1 root root 1.7M Sep  3 20:52 llama.cpp/models/ggml-vocab-refact.gguf\n",
            "-rw-r--r-- 1 root root 1.7M Sep  3 20:52 llama.cpp/models/ggml-vocab-starcoder.gguf\n",
            "-rw-r--r-- 1 root root 241M Sep  3 21:01 llama.cpp/models/gpt2-shrek-chaos.gguf\n",
            "-rw-r--r-- 1 root root  66M Sep  3 21:01 llama.cpp/models/gpt2-shrek-chaos-Q2_K.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./build/bin/llama-cli -m models/gpt2-shrek-chaos-Q2_K.gguf -p \"1+1 = \" -n 100 --temp 1 --no-warmup --repeat_penalty 2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4vBS8yQWYLd",
        "outputId": "f58e462a-63a1-491f-c9dc-5825bee3fde8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 6373 (0fce7a12) with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 148 tensors from models/gpt2-shrek-chaos-Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gpt2 Shrek Chaos\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 124M\n",
            "llama_model_loader: - kv   4:                           gpt2.block_count u32              = 12\n",
            "llama_model_loader: - kv   5:                        gpt2.context_length u32              = 1024\n",
            "llama_model_loader: - kv   6:                      gpt2.embedding_length u32              = 768\n",
            "llama_model_loader: - kv   7:                   gpt2.feed_forward_length u32              = 3072\n",
            "llama_model_loader: - kv   8:                  gpt2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv   9:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  11:                         tokenizer.ggml.pre str              = gpt-2\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,50257]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,50257]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 50256\n",
            "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  21:                          general.file_type u32              = 10\n",
            "llama_model_loader: - type  f32:   99 tensors\n",
            "llama_model_loader: - type q2_K:   24 tensors\n",
            "llama_model_loader: - type q3_K:   24 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q2_K - Medium\n",
            "print_info: file size   = 63.66 MiB (4.29 BPW) \n",
            "load: printing all EOG tokens:\n",
            "load:   - 50256 ('<|endoftext|>')\n",
            "load: special tokens cache size = 1\n",
            "load: token to piece cache size = 0.3060 MB\n",
            "print_info: arch             = gpt2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 1024\n",
            "print_info: n_embd           = 768\n",
            "print_info: n_layer          = 12\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 12\n",
            "print_info: n_rot            = 64\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 64\n",
            "print_info: n_embd_head_v    = 64\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 768\n",
            "print_info: n_embd_v_gqa     = 768\n",
            "print_info: f_norm_eps       = 1.0e-05\n",
            "print_info: f_norm_rms_eps   = 0.0e+00\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 3072\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = -1\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 1024\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 0.1B\n",
            "print_info: model params     = 124.44 M\n",
            "print_info: general.name     = Gpt2 Shrek Chaos\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 50257\n",
            "print_info: n_merges         = 50000\n",
            "print_info: BOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOT token        = 50256 '<|endoftext|>'\n",
            "print_info: UNK token        = 50256 '<|endoftext|>'\n",
            "print_info: PAD token        = 50256 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 50256 '<|endoftext|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors:   CPU_Mapped model buffer size =    63.66 MiB\n",
            "........................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = auto\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) > n_ctx_train (1024) -- possible training context overflow\n",
            "llama_context:        CPU  output buffer size =     0.19 MiB\n",
            "llama_kv_cache:        CPU KV buffer size =   144.00 MiB\n",
            "llama_kv_cache: size =  144.00 MiB (  4096 cells,  12 layers,  1/1 seqs), K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
            "llama_context: Flash Attention was auto, set to enabled\n",
            "llama_context:        CPU compute buffer size =    99.66 MiB\n",
            "llama_context: graph nodes  = 430\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: added <|endoftext|> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "main: llama threadpool init, n_threads = 1\n",
            "main: model was trained on only 1024 context tokens (4096 specified)\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "sampler seed: 1898903869\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 2.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 1.000\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 100, n_keep = 0\n",
            "\n",
            "1+1 = 　. . 1-2: ⊙ The Earth is his own mirror, so I will grant me permission to use my name if you wish (like us) on your planet [the] i want it for that future 😂😔👈🏻‍(our time frame). My birth date was 12 years ago and this has been forever in the making. It's a testament of how happy I feel when we are together, sharing is good sense ❉️\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =      79.16 ms /   105 runs   (    0.75 ms per token,  1326.43 tokens per second)\n",
            "llama_perf_context_print:        load time =     239.28 ms\n",
            "llama_perf_context_print: prompt eval time =      34.50 ms /     5 tokens (    6.90 ms per token,   144.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =    1268.03 ms /    99 runs   (   12.81 ms per token,    78.07 tokens per second)\n",
            "llama_perf_context_print:       total time =    1599.29 ms /   104 tokens\n",
            "llama_perf_context_print:    graphs reused =         98\n"
          ]
        }
      ]
    }
  ]
}